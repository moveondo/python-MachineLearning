
### KNN VS K-means

K近邻法（knn）是一种基本的分类与回归方法。

k-means是一种简单而有效的聚类方法。

虽然两者用途不同、解决的问题不同，但是在算法上有很多相似性，于是将二者放在一起，这样能够更好地对比二者的异同。

二者的相同点:

- k的选择类似

- 思路类似：根据最近的样本来判断某个样本的属性

二者的不同点：

应用场景不同：前者是分类或者回归问题，后者是聚类问题;

算法复杂度： 前者O（n^2）,后者O（kmn）;（k是聚类类别数，m是聚类次数）

稳定性：前者稳定，后者不稳定。

 ![Image text](https://github.com/moveondo/python-MachineLearning/blob/master/%E7%AE%97%E6%B3%95%E9%97%B4%E5%8C%BA%E5%88%AB%E8%81%94%E7%B3%BB/image/knnkmean.jpg)


### Apriori VS FP-growth


频繁项集：是经常出现在一块儿的物品的集合
关联规则：按时两种物品之间可能存在很强的关系。

支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小值尺度的项集。

可信度或者是置信度是针对关联规则来定义的，我们的规则对其中多少的记录都适用

Apriori算法是发现频繁项集的一种方法，Apriori算法的两个输入参数分别是最小支持度和数据集，该算法首先会生成所有单个元素的项集列表。接着扫描数据集来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉，然后，对剩下来的集合进行组合以生成包含两个元素的项集;
接下来，再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集都被去掉。

经典的关联规则挖掘算法包括Apriori算法和FP-growth算法。
apriori算法多次扫描交易数据库，每次利用候选频繁集产生频繁集；

而FP-growth则利用树形结构，无需产生候选频繁集而是直接得到频繁集，大大减少扫描交易数据库的次数，从而提高了算法的效率。
但是apriori的算法扩展性较好，可以用于并行计算等领域。

使用Apriori算法进行关联分析。FP-growth算法来高效发现频繁项集。



### LR和SVM的联系与区别？

@朝阳在望，联系： 
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 

2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。 

区别： 

1、LR是参数模型，SVM是非参数模型。 

2、从目标函数来看，区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 

3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 

4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 

5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。




### LR与线性回归的区别与联系？

个人感觉逻辑回归和线性回归首先都是广义的线性回归， 

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数， 

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 

@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。




### Q:在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？

曼哈顿距离只计算水平或垂直距离，有维度的限制。

另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。




1.训练决策树时的参数是什么？

* 1.criterion gini(基尼系数) or entropy(信息熵)  

* 2.splitter best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）

* 3.max_features None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的

* 4.max_depth 数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下

* 5.min_samples_split 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

* 6.min_samples_leaf 这个值限制了叶子节点最少的样本数，如果某叶子节点样本数目小于min_samples_leaf，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

* 7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

* 8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。

* 9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。

* 10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。

 

2.在决策树的节点处分割的标准是什么？



3.基尼系数的公式是什么？

G=A/(A+B）

赫希曼根据洛伦茨曲线提出的判断分配平等程度的指标。设实际收入分配曲线和收入分配绝对平等曲线之间的面积为A，实际收入分配曲线右下方的面积为B。并以A除以（A+B）的商表示不平等程度。这个数值被称为基尼系数或称洛伦茨系数。如果A为零，基尼系数为零，表示收入分配完全平等；如果B为零则系数为1，收入分配绝对不平等。
收入分配越是趋向平等，洛伦茨曲线的弧度越小，基尼系数也越小，反之，收入分配越是趋向不平等，洛伦茨曲线的弧度越大，那么基尼系数也越大。另外，可以参看帕累托指数(是指对收入分布不均衡的程度的度量）。



4.熵的公式是什么？

计算公式

H(x) = E[I(xi)] = E[ log(2,1/p(xi)) ] = -∑p(xi)log(2,p(xi)) (i=1,2,..n)

其中，x表示随机变量，与之相对应的是所有可能输出的集合，定义为符号集,随机变量的输出用x表示。P(x)表示输出概率函数。变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大.



5.决策树如何决定在哪个特征处分割？



  那么熵如何影响决策树绘制边界呢？

  这里涉及到一个新的词汇：信息增益

  信息增益定义为：父项熵-加权平均值（分割父项熵后生成的子项熵）

  决策树算法会最大程度的提高信息增益，他通过这种算法来选择进行分割的特征。如果特征有多个可获取的不同值，决策树会通过尽可能提高信息增益的办法决定在何处分割





6.你如何用数学计算收集来的信息？你确定吗？

7.随机森林的优点有哪些？

8.介绍一下boosting算法。

9.gradient boosting如何工作？

10.关于AdaBoost算法，你了解多少？它如何工作？

11.SVM中用到了哪些核？SVM中的优化技术有哪些？

12.SVM如何学习超平面？用数学方法详细解释一下。

13.介绍一下无监督学习，算法有哪些？

14.在K-Means聚类算法中，如何定义K？

法1：(轮廓系数)在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目


15.告诉我至少3中定义K的方法。



