
### KNN VS K-means

K近邻法（knn）是一种基本的分类与回归方法。

k-means是一种简单而有效的聚类方法。

虽然两者用途不同、解决的问题不同，但是在算法上有很多相似性，于是将二者放在一起，这样能够更好地对比二者的异同。

二者的相同点:

- k的选择类似

- 思路类似：根据最近的样本来判断某个样本的属性

二者的不同点：

应用场景不同：前者是分类或者回归问题，后者是聚类问题;

算法复杂度： 前者O（n^2）,后者O（kmn）;（k是聚类类别数，m是聚类次数）

稳定性：前者稳定，后者不稳定。

 ![Image text](https://github.com/moveondo/python-MachineLearning/blob/master/%E7%AE%97%E6%B3%95%E9%97%B4%E5%8C%BA%E5%88%AB%E8%81%94%E7%B3%BB/image/knnkmean.jpg)


### Apriori VS FP-growth


频繁项集：是经常出现在一块儿的物品的集合
关联规则：按时两种物品之间可能存在很强的关系。

支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小值尺度的项集。

可信度或者是置信度是针对关联规则来定义的，我们的规则对其中多少的记录都适用

Apriori算法是发现频繁项集的一种方法，Apriori算法的两个输入参数分别是最小支持度和数据集，该算法首先会生成所有单个元素的项集列表。接着扫描数据集来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉，然后，对剩下来的集合进行组合以生成包含两个元素的项集;
接下来，再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集都被去掉。

经典的关联规则挖掘算法包括Apriori算法和FP-growth算法。
apriori算法多次扫描交易数据库，每次利用候选频繁集产生频繁集；

而FP-growth则利用树形结构，无需产生候选频繁集而是直接得到频繁集，大大减少扫描交易数据库的次数，从而提高了算法的效率。
但是apriori的算法扩展性较好，可以用于并行计算等领域。

使用Apriori算法进行关联分析。FP-growth算法来高效发现频繁项集。


Q:在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？

曼哈顿距离只计算水平或垂直距离，有维度的限制。

另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。


### LR和SVM的联系与区别？

@朝阳在望，联系： 
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 

2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。 

区别： 

1、LR是参数模型，SVM是非参数模型。 

2、从目标函数来看，区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 

3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 

4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 

5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。




### LR与线性回归的区别与联系？

个人感觉逻辑回归和线性回归首先都是广义的线性回归， 

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数， 

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 

@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。


