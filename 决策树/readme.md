### 决策树算法


### 决策树的定义：

 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。
内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。

  用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。
如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。


### 决策树的构造方法和步骤

决策树的生成是一个从根节点开始，从上到下的递归过程。

一般采用分而治之的方法，通过不断地将训练样本划分成子集来构造决策树。 

假设给定的训练集 T 总共有 m 个类别。则针对 T 构造决策树时，会出现以下三种情况:

(1) 如果 T 中所有样本的类别相同，那么决策树只有一个叶子结点。

(2) 如果 T 中没有可用于继续分裂的变量，则将 T 中出现频率最高的类别作为当前结点的类别。

(3) 如果 T 包含的样本属于不同的类别，根据变量选择策略，选择最佳的变量和划分方式将 T 分为几个子集 T1,T2,...,Tk， 每个数据子集构成一个内部结点。
 对于某个内部结点继续进行判断，重复上述操作，直到满足决策树的终止条件为止。
 终止条件就是,结点对应的所有样本 属于同一个类别，或者 T 中没有可用于进—步分裂的变量。
 
 
### 决策树构建算法 Generate_decision_tree。

输入:训练集 T，输入变量集 A，目标(类别)变量 Y

输出:决策树 Tree

Generate_decision_tree(T,A,Y)

1:如果 T 为空，返回出错信息;

2:如果 T 的所有样本都属于同一个类别 C，则用 C 标识当前节点并返回;

3:如果没有可分的变量，则用 T 中出现频率最高的类别标识当前结点并返回;

4:根据变量选择策略选择最佳变量 X 将 T 分为 k 个子集(T1，T2，...Tk);

5:用 X 标识当前结点;

6:对 T 的每一个子集 Ti

7:NewNode= Generate_decision_tree(Ti,A-X,Y); //递归操作

8:生成一个分枝，该分枝由结点 X 指向 NewNode;

9:返回当前结点。

在上述算法中，结点分裂(第 4 步)是生成决策树的重要步骤。
只有根据不同的变量将单个结点分裂成多个结点，方能形成多个类别，因此整个问题的核心就是如何选择分裂变量。


### 决策树 原理

#### 决策树 须知概念

信息熵 & 信息增益

熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。

信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。

信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。


#### 决策树 工作原理

如何构造一个决策树?

我们使用 createBranch() 方法，如下所示：

```
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点


```

#### 决策树 开发流程

> 收集数据：可以使用任何方法。
> 准备数据：树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
>分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
>训练算法：构造树的数据结构。
>测试算法：使用训练好的树计算错误率。
>使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。

#### 决策树 算法特点

* 优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。
* 缺点：容易过拟合。
* 适用数据类型：数值型和标称型。

