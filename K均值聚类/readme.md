### K-Means（K-均值）聚类算法

> [定义] 聚类是根据数据的特征将数据分组，使具有最大的组内相似性和最小的组间相似性，是无监督学习。

1、聚类的两种方法

(1)基于相似度法(硬聚类):K-Means，BRICH 等

[基于相似度方法] 适合于大数据量聚类的 BRICH 算法，有 3 大贡献

 * 1 提出了聚类特征 CF(Clustering Feature) 
 * 2 聚类特征 CF 满足线性可加性 
 * 3 用 B-tree 的性质实现了 CF-tree 基于密度法(硬聚类):DBSCAN，OPTICS(基于密度也可以理解成基于相似度)

(2)基于模型法即概率法(软聚类):GMM 等

2、对象间的相似性是聚类分析的核心
 (1)对象的属性分类
  * 1)区间标度型变量 
  * 2)二元变量(0 或 1) 
  * 3)分类型变量 
  * 4)序数型变量 
  * 5)比例型变量 
  * 6)混合型 

(2)区间标度型对象之间的相似度(或相异度)计算

是基于对象间距离来计算的，通常用:1名考斯基距离，2当 q=1 时,d 称为曼哈顿距离;3当 q=2 时,d 称为欧几里得距离

![Image text](https://github.com/moveondo/python-MachineLearning/blob/master/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/image/k-mean.jpg)

(最常用)曼哈顿函数表示两个点在标准坐标系上的绝对轴距总和，坐标(x1, y1)的 i 点与坐标(x2, y2)的 j 点的 曼哈顿距离为:d(i,j)=|X1-X2|+|Y1-Y2|。找曼哈顿距离最近的那个点就是新的中心点。


3、主要的聚类算法(5 种)

* 1)基于划分的算法:给定一个 n 个对象或元组的数据库，将数据划分为 k 个组(k 是事先给定的，k<=n)。如 K-Means [定义] K-Means 方法是 MacQueen1967 年提出的。给定一个数据集合 X 和一个整数 K(<=n)，K-Means 方法是将 X 分成 K 个 聚类并使得在每个聚类中所有值与该聚类中心距离的总和最小。 

* 2)基于层次的算法:是把数据对象排列成一个聚类树，在需要的层次上对其进行切割，关联的部分构成一个 cluster。有两种类型 1聚合层次聚类; 2划分层次聚类。如 BRICH
 
* 3)基于密度的算法:绝大多数划分方法基于对象之间的距离进行聚类，只能发现凸状的簇。如 DBSCAN，OPTICS 

* 4)基于方格的算法:把多维空间划分成一定数目的单元(cell)，然后在这种数据结构上进行聚类操作，速度快,但不精确 

* 5)基于模型的算法:1基于神经网络的方法 2 基于概率统计的方法，典型方法有高斯混合模型 GMM(Gaussian Mixture Models)，典型应用有话题检测(EM)


4、好的聚类算法特点 Requirements of Clustering in Data Mining

 * 可伸缩性
 * 能够处理各种不同类型的属性
 * 能够发现任意形状的聚类
 * 在决定输入参数的时候，对领域知识的需求要小  能够处理噪声和异常点
 * 对输入数据的顺序不敏感
 * 可以处理高维数据
 * 可以和用户制定的限定条件相结合  可解释性和使用性好



聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。

至于“相似”这一概念，是利用距离这个评价标准来衡量的，我们通过计算对象与对象之间的距离远近来判断它们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中欧式距离是最为简单和常用的，除此之外还有曼哈顿距离和余弦相似性距离等。

欧式距离，我想大家再熟悉不过了，但为免有一些基础薄弱的同学，在此再说明一下，它的定义为：
对于x点(坐标为(x1,x2,x3,...,xn))和 y点（坐标为(y1,y2,y3,...,yn)），两者的欧式距离为
 
 d(x,y)={\sqrt  {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}={\sqrt  {\sum {{i=1}}^{n}(x{i}-y_{i})^{2}}}
 
在二维平面，它就是我们初中时就学过的两点距离公式

### K-Means 算法

K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 K-均值 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.
簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.

>聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.

优点:

* 属于无监督学习，无须准备训练集
* 原理简单，实现起来较为容易
* 结果可解释性较好

缺点:

* 需手动设置k值。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性
* 可能收敛到局部最小值, 在大规模数据集上收敛较慢
* 对于异常点、离群点敏感
* 使用数据类型:数值型数据

### K-Means 场景

kmeans，如前所述，用于数据集内种类属性不明晰，希望能够通过数据挖掘出或自动归类出有相似特点的对象的场景。其商业界的应用场景一般为挖掘出具有相似特点的潜在客户群体以便公司能够重点研究、对症下药。

例如，在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为50.7%而最小百分比为47.9% 如果1%的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。 实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？ 答案就是聚类（Clustering)。

那么，具体如何实施呢？首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇 ）， 精心构造能够吸引该簇选民的消息。最后， 开展竞选活动并观察上述做法是否有效。

另一个例子就是产品部门的市场调研了。为了更好的了解自己的用户，产品部门可以采用聚类的方法得到不同特征的用户群体，然后针对不同的用户群体可以对症下药，为他们提供更加精准有效的服务。

### K-Means 术语

* 簇: 所有数据的点集合，簇中的对象是相似的。

* 质心: 簇中所有点的中心（计算所有点的均值而来）.

* SSE: Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心. 聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）。详情见kmeans的评价标准。



### K-Means 工作流程

首先, 随机确定 K 个初始点作为质心（不必是数据中的点）。

然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值. 3.重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。
上述过程的 伪代码 如下:

  * 创建 k 个点作为起始质心（通常是随机选择）
  * 当任意一个点的簇分配结果发生改变时（不改变时算法结束）
    * 对数据集中的每个数据点
      * 对每个质心
        * 计算质心与数据点之间的距离
      * 将数据点分配到距其最近的簇
  * 对每一个簇, 计算簇中所有点的均值并将均值作为质心
  
### K-Means 开发流程

```
收集数据：使用任意方法
准备数据：需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算
分析数据：使用任意方法
训练算法：不适用于无监督学习，即无监督学习不需要训练步骤
测试算法：应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.
使用算法：可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.

```

### K-Means 的评价标准

k-means算法因为手动选取k值和初始化随机质心的缘故，每一次的结果不会完全一样，而且由于手动选取k值，我们需要知道我们选取的k值是否合理，聚类效果好不好，那么如何来评价某一次的聚类效果呢？也许将它们画在图上直接观察是最好的办法，但现实是，我们的数据不会仅仅只有两个特征，一般来说都有十几个特征，而观察十几维的空间对我们来说是一个无法完成的任务。
因此，我们需要一个公式来帮助我们判断聚类的性能，这个公式就是SSE (Sum of Squared Error, 误差平方和 ），它其实就是每一个点到其簇内质心的距离的平方值的总和，这个数值对应kmeans函数中clusterAssment矩阵的第一列之和。 SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。
聚类的目标是在保持簇数目不变的情况下提高簇的质量。


### 两道试题：


![Image text](https://github.com/moveondo/python-MachineLearning/blob/master/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/image/kmean1.jpg)


![Image text](https://github.com/moveondo/python-MachineLearning/blob/master/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/image/kmean2.jpg)


### K-Means 聚类算法的缺陷

>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）.


出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。

为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大SSE值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，令k设为2。

为了保持簇总数不变，可以将某两个簇进行合并。从上图中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？

有两种可以量化的办法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。

因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.

#### 二分 K-Means 聚类算法

该算法首先将所有点作为一个簇，然后将该簇一分为二。

之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。

上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。

#### 二分 K-Means 聚类算法伪代码

* 将所有点看成一个簇

* 当簇数目小于 k 时'

* 对于每一个簇
  * 计算总误差
  * 在给定的簇上面进行 KMeans 聚类（k=2）
  * 计算将该簇一分为二之后的总误差
* 选择使得误差最小的那个簇进行划分操作

另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。
